{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = 1\n",
    "        self.n_layers = 1\n",
    "        self.batch_size = 1\n",
    "        \n",
    "        self.gru = nn.GRU(10,self.hidden_dim,self.n_layers)#dummy numbers, figure it later\n",
    "        self.fc = nn.Linear(self.hidden_dim,3)\n",
    "        \n",
    "\n",
    "    def forward(self, embedded):\n",
    "        print(embedded)\n",
    "        print(embedded.shape)\n",
    "        hidden = self.init_hidden()\n",
    "        out, hidden = self.gru(embedded, hidden)\n",
    "        print(out)\n",
    "        print(out.shape)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, self.batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):#not sure if this is necessary anymore since im padding w/ different script\n",
    "    batch_body = [t[0] for t in batch]\n",
    "    batch_y = [t[1] for t in batch]\n",
    "\n",
    "    lengths_x1 = torch.tensor([t.shape[0] for t in batch_body])\n",
    "    b_x1 = pad_sequence(batch_body, batch_first=True, padding_value=0)\n",
    "    batch_y = torch.tensor(batch_y)\n",
    "\n",
    "    return b_x1, lengths_x1, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataLoader):\n",
    "    acc = 0\n",
    "    for i, data in enumerate(dataLoader):\n",
    "        text = data[0]\n",
    "        label = data[1]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = loss(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc += (output.argmax(1) == label).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "    acc = acc/len(dataLoader)\n",
    "    print(f'Train Accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanLine(x):\n",
    "    x = x[1:-1]\n",
    "    x = re.findall(r\"('.*?')\", x)\n",
    "    x = [i[1:-1] for i in x]\n",
    "    return x\n",
    "\n",
    "def splitLen(x):\n",
    "    try:\n",
    "        return len(x.split(' '))\n",
    "        \n",
    "    except:\n",
    "        print(f\"ERROR got: {x}\")\n",
    "        \n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, csv, bodyNumpyList):\n",
    "        df = pd.read_csv(csv)\n",
    "        #self.body = pickle.load(bodyNumpyList)#this is a list of numpy arrays that have 0 padding at the beginning\n",
    "        self.body = df['BodyCleaned']#.apply(lambda x: cleanLine(x))\n",
    "        self.body.replace('', np.nan, inplace = True)\n",
    "        self.body.dropna(inplace = True)\n",
    "        maxLen =  df['BodyCleaned'].apply(splitLen).max()\n",
    "        for idx, items in enumerate(df['BodyCleaned'].iteritems()): \n",
    "            if isinstance(items[1], float):\n",
    "                print(idx, items[1])\n",
    "        print(maxLen)\n",
    "        uniqueList = []\n",
    "        for line in self.body:\n",
    "            for u in np.unique(line):\n",
    "                uniqueList.append(u)\n",
    "        uniqueList = np.array(uniqueList)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit_transform(np.unique(uniqueList))\n",
    "        for i in self.body:\n",
    "            print(i)\n",
    "            print(type(i))\n",
    "            break\n",
    "        self.body = [np.concatenate(np.zeros(maxLen - len(i), 1), label_encoder.transform(i.split(' '))) for i in self.body]\n",
    "        self.label = df['Y'].values.tolist()\n",
    "\n",
    "        self.length = len(df)\n",
    "       \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        print(self.body[index])\n",
    "        toReturn = [torch.tensor(self.body[index]), torch.tensor(np.array(self.label[index]))]\n",
    "        return toReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19424\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.71 GiB for an array with shape (44996,) and data type <U28086",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-80ebcf8df280>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrainData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train-cleaned.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bodyVectorized.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#dummy value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtrainDataLoader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                   \u001b[1;31m#collate_fn=generate_batch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-1fa11d23b65e>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, csv, bodyNumpyList)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0muniqueList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniqueList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_encoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniqueList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\plupi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\plupi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.71 GiB for an array with shape (44996,) and data type <U28086"
     ]
    }
   ],
   "source": [
    "EPOCH = 4\n",
    "model = network()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "trainData = myDataset('train-cleaned.csv', 'bodyVectorized.pkl')#dummy value\n",
    "trainDataLoader = DataLoader(trainData, batch_size=8, shuffle=True)\n",
    "                  #collate_fn=generate_batch)\n",
    "for _ in range(EPOCH):\n",
    "    train(model, trainDataLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testData = TensorDataset(torch.from_numpy(trainX), torch.from_numpy(trainY))\n",
    "testData  = myDataset('test-vectorized.csv', 'pklPath.pkl')#dummy values\n",
    "testDataLoader = DataLoader(testData, batch_size=8, shuffle=True)\n",
    "#                  collate_fn=generate_batch)\n",
    "with torch.no_grad():\n",
    "    acc = 0\n",
    "    for data in testDataLoader:#[text, label]\n",
    "        text = data[0]\n",
    "        label = data[1]\n",
    "        output = model(text)\n",
    "        acc += (output.argmax(1) == label).sum().item()\n",
    "    print(f'Testing accuracy: {acc/len(trainDataLoader)}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
